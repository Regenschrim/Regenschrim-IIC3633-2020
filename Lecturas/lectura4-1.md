# Lectura 4 - 1

A grandes rasgos el paper habla sobre content-based recommendations y hace enfasis en varias de las técnicas que han existido a lo largo de los años y las ideas principales que hay detrás de estos técnicas. En partícular se centra mucho en la clasificación de textos y cómo se crean datos que representan a los textos (o ítems), y a los usuarios, ya sea aprendiendo las características de estos a través de *implicit* o *explicit feedback* o de una caracterízación del mismo usuario.

No se entra en muchos detalles de acuerdo a la matemática dentro de estas técnicas y en algunos casos habla de la intuición que hay detrás de ciertas fórmular, pero además de su falta de pruebas empíricas respecto a la intuición.

En partícular me causa mucho ruido el hecho de que en la sección donde se habla de la caracterización del usuario el paper sea tan tajante en el sentido de que al usuario le gusta o no le gusta algo, y no deja abierta la posibilidad a que hayan otras razones detrás de la "no preferencia". Sobre todo cuando pone el ejemplo de que un usuario devuelve un ítem debido a que no le gustó y no pudo haber sido porque estaba en mal estado o alguna otra razón. Quizás ciertos grados de confianza podrían ser una alternativa a esa rígida clasificación de gustos (aunque probablemente complique más el modelo).

Por otro lado, a pesar de que esté de acuerdo con muchos de los razonamientos e intuiciones presentados para algunos algoritmos, creo que los modelos probabilísticos o de Bayes, a pesar de su rendimiento, podrían tener un approach distinto. En estos algoritmos se suele asumir una independencia entre palabras juntas dentro del texto. Sin embargo, teniendo en consideración la formulación de una oración dentro del lenguaje (teniendo en consideración que no todos los idiomas funcionan igual y quizás sea aplicable a algunos) y que el algoritmo funciona con un vocabulario *V*, se podría hacer *clusters* de palabras de la forma: adjetivos, sustantivos, conectores, pronombres, etc. De esta forma dada una palabra o varias palabras dentro de un texto se podría calcular una probabilidad condicionada respecto a las palabras que estaban antes de la palabra que se esta viendo actualmente. Por ejemplo, podríamos decir que hace más sentido tener una probabilidad más alta de tener un adjetivo luego de un sustantivo que otro sustantivo. De esta forma podría intentar mejorar el rendimiento del algoritmo y quizás con una validación cruzada o algún método similar, encontrar cuantas palabras tener en consideración para calcular la probabilidad condicional de la aparición de una palabra.
